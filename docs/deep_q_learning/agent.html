<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>deep_q_learning.agent API documentation</title>
<meta name="description" content="Implementation of the agent classes and associated RL algorithms." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>deep_q_learning.agent</code></h1>
</header>
<section id="section-intro">
<p>Implementation of the agent classes and associated RL algorithms.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Implementation of the agent classes and associated RL algorithms.

&#34;&#34;&#34;
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import namedtuple, deque
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple
# Named tuple for replay buffer storage
Transition = namedtuple(&#39;Transition&#39;,
                        (&#39;state&#39;, &#39;action&#39;, &#39;next_state&#39;, &#39;reward&#39;))


class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        &#34;&#34;&#34;Save a transition&#34;&#34;&#34;
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class Agent(ABC):
    &#34;&#34;&#34;Implements acting and learning. (Abstract class, for implementations see DQNAgent and NaiveAgent).

    Args:
        ABC (_type_): _description_

    Returns:
        _type_: _description_
    &#34;&#34;&#34;
    @abstractmethod
    def __init__(self,  env, *args, **kwargs):
        &#34;&#34;&#34;
        Args:
            env (_type_): the simulation environment
        &#34;&#34;&#34;
        
    @abstractmethod
    def load_model(self, savepath:str):
        &#34;&#34;&#34;Loads weights from a file.

        Args:
            savepath (str): path at which weights are saved.
        &#34;&#34;&#34;
        
    @abstractmethod
    def save_model(self, savepath:str):
        &#34;&#34;&#34;Saves weights to a specified path

        Args:
            savepath (str): the path
        &#34;&#34;&#34;
        
    @abstractmethod
    def optimize_model(self)-&gt;float:
        &#34;&#34;&#34;Perform one optimization step.

        Returns:
            float: the loss
        &#34;&#34;&#34;
    
    @abstractmethod
    def reset():
        &#34;&#34;&#34;Resets the agent&#39;s inner state
        &#34;&#34;&#34;
        
    @abstractmethod 
    def act(self, obs:torch.Tensor)-&gt;Tuple[int, float]:
        &#34;&#34;&#34;Selects an action based on an observation.

        Args:
            obs (torch.Tensor): an observation

        Returns:
            Tuple[int, float]: the selected action (as an int) and associated Q/V-value as a float
        &#34;&#34;&#34;

class DQNAgent(Agent):
    &#34;&#34;&#34;Implements acting and learning using deep Q-learning (see [the DQN paper](https://arxiv.org/pdf/1312.5602.pdf)).  
    
    Q-learning aims to optmizes an agent&#39;s policy by maximizing the Bellman equation:
    $$
    Q^*(s,a) = \mathbb{E}_{s&#39; \sim \mathcal{E}} [r+\gamma \max_{a&#39;}Q^*(s&#39;,a&#39;)|s,a]
    $$
    
    In the case of Deep Q-Learning this is performed by minizing a sequence of loss functions \(L_i(\\theta_i)\) which change at each iteration \(i\) of the algorithm:
    $$
    L_i(\\theta_i) = \mathbb{E}_{s&#39;, a \sim \\rho(\cdot)} [ g(y_i - Q(s,a;\\theta_i)) ]
    $$
    where \(g : \mathbb{R}^n\\rightarrow\mathbb{R}\) is some loss function, usually [L2](https://en.wikipedia.org/wiki/Mean_squared_error) or [Huber-loss](https://en.wikipedia.org/wiki/Huber_loss) and \(y_i= \mathbb{E}_{s&#39; \sim \\rho(\cdot)} [r + \gamma \max_{a&#39;} Q(s&#39;,a&#39;;\\theta_{i-1}) ]\) is the *target*. The policy is updated with stochastic gradient descent where stochastic gardients are sampled from the following full gradient computation:
    $$
    \\nabla L_i(\\theta_i) = \mathbb{E}_{s&#39; \sim \epsilon} [ g&#39;(r + \gamma \max_{a&#39;} Q(s&#39;,a&#39;;\\theta_{i-1})-  Q(s,a;\\theta_i)) \cdot \\nabla_{\\theta_i} Q(s,a;\\theta_i)) ]
    $$
    
    
    &#34;&#34;&#34;

    def __init__(self,  env,
                 model:torch.nn,
                 criterion=nn.HuberLoss(),
                 lr:float=5e-4,
                 epsilon:float=0.5,
                 gamma:float=0.99,
                 buffer_size:int=10000,
                 batch_size:int=64):
        &#34;&#34;&#34;

        Args:
            env (_type_): the simulation environment.
            model (_type_): the torch module to use for learning
            criterion (nn._Loss, optional): the loss function. Defaults to nn.HuberLoss().
            lr (float, optional): DQN&#39;s learning rate. Defaults to 5e-4.
            epsilon (float, optional): the exploration ratio epsilon (see above). Defaults to 0.5.
            gamma (float, optional): the gamma term (see above). Defaults to 0.99.
            buffer_size (int, optional): the replay buffer size. Defaults to 10000.
            batch_size (int, optional): the size of each training batch. Defaults to 64.
        &#34;&#34;&#34;

        self.env = env

        model_params = {
            &#39;in_dim&#39;: len(env.observation_space.sample().flatten()),
            &#39;out_dim&#39;: env.action_space.n,
        }
        self.model = model(**model_params)
        self.targetModel = model(**model_params)

        self.criterion = criterion

        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)

        self.device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        self.memory = ReplayMemory(buffer_size)
        self.batch_size = batch_size

        self.epsilon = epsilon
        self.gamma = gamma
        self.lr = lr

    def load_model(self, savepath):
        torch.save(self.model.state_dict(), savepath)
        self.model.load_state_dict(torch.load(savepath))

    def save_model(self, savepath):
        torch.save(self.model.state_dict(), savepath)

    def optimize_model(self):
        if len(self.memory) &lt; self.batch_size:
            return np.double(0)

        # Sample memory
        transitions = self.memory.sample(self.batch_size)

        # Convert Batch(Transitions) -&gt; Transition(Batch)
        batch = Transition(*zip(*transitions))

        action_batch = torch.tensor([e for e in batch.action])
        state_batch = torch.cat(batch.state, 0)
        next_states_batch = torch.cat(batch.next_state, 0)
        reward_batch = torch.cat(batch.reward)

        # Compute Q(S, a) with the Q-value network
        state_action_values = self.model(
            state_batch).gather(1, action_batch.unsqueeze(1))

        # Compute max_ap Q(Sp) with the stable target network
        next_state_values = self.targetModel(next_states_batch).max(1)[
            0].detach().unsqueeze(1)
        # Compute the expected Q values
        expected_state_action_values = (
            next_state_values * self.gamma) + reward_batch

        # Compute Huber loss
        loss = self.criterion(state_action_values,
                              expected_state_action_values)

        # Optimize the model
        self.optimizer.zero_grad()
        loss.backward()
        for param in self.model.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()

        return np.double(loss)
    
    def reset():
        pass # Not stateful

    def act(self, obs):
        x = torch.Tensor(obs)

        epsilon = self.epsilon
        sample = random.random()

        Q_est = self.model(x)
        Q = float(Q_est.detach().max())
        if sample &gt; epsilon:
            with torch.no_grad():
                return np.argmax(
                    np.exp(Q_est)), Q
        else:
            return self.env.action_space.sample(), Q


class NaiveAgent(Agent):

    def __init__(self,  env,
                 threshold:int=20000,
                 confine_time_int=4,):
        &#34;&#34;&#34;Naive Agent implementation. Gives a baseline to compare reinforcement learning agents against. 
        The naive policy is the following:
        ```pseudocode
        If number of infected people &gt; THRESHOLD
            confine the entire country for CONFINEMENT_TIME weeks
        ```

        Args:
            env (_type_): the simulation environment.
            threshold (int, optional): The infected threshold, upon which confiment must start. Defaults to 20000.
            confine_time (int, optional): The confinement time. Defaults to 4.
        &#34;&#34;&#34;

        self.env = env
        self.threshold = threshold
        self.confine_time = confine_time
        self.timer = 0
    def load_model(self, savepath):
        pass

    def save_model(self, savepath):
        pass

    def optimize_model(self):
        #This is agent is born stupid and stays stupid
        return 0
    
    def reset(self,):
        self.timer = 0

    def act(self, obs):
        if self.timer &gt; 0:
            self.timer -=1
            return 1, 0
        if obs &gt; self.threshold:
            self.timer = self.confine_time
            return 1, 0
        return 0,0</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="deep_q_learning.agent.Agent"><code class="flex name class">
<span>class <span class="ident">Agent</span></span>
<span>(</span><span>env, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements acting and learning. (Abstract class, for implementations see DQNAgent and NaiveAgent).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ABC</code></strong> :&ensp;<code>_type_</code></dt>
<dd><em>description</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>_type_</code></dt>
<dd>the simulation environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Agent(ABC):
    &#34;&#34;&#34;Implements acting and learning. (Abstract class, for implementations see DQNAgent and NaiveAgent).

    Args:
        ABC (_type_): _description_

    Returns:
        _type_: _description_
    &#34;&#34;&#34;
    @abstractmethod
    def __init__(self,  env, *args, **kwargs):
        &#34;&#34;&#34;
        Args:
            env (_type_): the simulation environment
        &#34;&#34;&#34;
        
    @abstractmethod
    def load_model(self, savepath:str):
        &#34;&#34;&#34;Loads weights from a file.

        Args:
            savepath (str): path at which weights are saved.
        &#34;&#34;&#34;
        
    @abstractmethod
    def save_model(self, savepath:str):
        &#34;&#34;&#34;Saves weights to a specified path

        Args:
            savepath (str): the path
        &#34;&#34;&#34;
        
    @abstractmethod
    def optimize_model(self)-&gt;float:
        &#34;&#34;&#34;Perform one optimization step.

        Returns:
            float: the loss
        &#34;&#34;&#34;
    
    @abstractmethod
    def reset():
        &#34;&#34;&#34;Resets the agent&#39;s inner state
        &#34;&#34;&#34;
        
    @abstractmethod 
    def act(self, obs:torch.Tensor)-&gt;Tuple[int, float]:
        &#34;&#34;&#34;Selects an action based on an observation.

        Args:
            obs (torch.Tensor): an observation

        Returns:
            Tuple[int, float]: the selected action (as an int) and associated Q/V-value as a float
        &#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="deep_q_learning.agent.DQNAgent" href="#deep_q_learning.agent.DQNAgent">DQNAgent</a></li>
<li><a title="deep_q_learning.agent.NaiveAgent" href="#deep_q_learning.agent.NaiveAgent">NaiveAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deep_q_learning.agent.Agent.act"><code class="name flex">
<span>def <span class="ident">act</span></span>(<span>self, obs: torch.Tensor) ‑> Tuple[int, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Selects an action based on an observation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>an observation</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[int, float]</code></dt>
<dd>the selected action (as an int) and associated Q/V-value as a float</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod 
def act(self, obs:torch.Tensor)-&gt;Tuple[int, float]:
    &#34;&#34;&#34;Selects an action based on an observation.

    Args:
        obs (torch.Tensor): an observation

    Returns:
        Tuple[int, float]: the selected action (as an int) and associated Q/V-value as a float
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="deep_q_learning.agent.Agent.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self, savepath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads weights from a file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>savepath</code></strong> :&ensp;<code>str</code></dt>
<dd>path at which weights are saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def load_model(self, savepath:str):
    &#34;&#34;&#34;Loads weights from a file.

    Args:
        savepath (str): path at which weights are saved.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="deep_q_learning.agent.Agent.optimize_model"><code class="name flex">
<span>def <span class="ident">optimize_model</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Perform one optimization step.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def optimize_model(self)-&gt;float:
    &#34;&#34;&#34;Perform one optimization step.

    Returns:
        float: the loss
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="deep_q_learning.agent.Agent.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the agent's inner state</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def reset():
    &#34;&#34;&#34;Resets the agent&#39;s inner state
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="deep_q_learning.agent.Agent.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self, savepath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves weights to a specified path</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>savepath</code></strong> :&ensp;<code>str</code></dt>
<dd>the path</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def save_model(self, savepath:str):
    &#34;&#34;&#34;Saves weights to a specified path

    Args:
        savepath (str): the path
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="deep_q_learning.agent.DQNAgent"><code class="flex name class">
<span>class <span class="ident">DQNAgent</span></span>
<span>(</span><span>env, model: <module 'torch.nn' from '/Users/renard/miniconda3/envs/DeepLearning3.9/lib/python3.9/site-packages/torch/nn/__init__.py'>, criterion=HuberLoss(), lr: float = 0.0005, epsilon: float = 0.5, gamma: float = 0.99, buffer_size: int = 10000, batch_size: int = 64)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements acting and learning using deep Q-learning (see <a href="https://arxiv.org/pdf/1312.5602.pdf">the DQN paper</a>).
</p>
<p>Q-learning aims to optmizes an agent's policy by maximizing the Bellman equation:
<span><span class="MathJax_Preview">
Q^*(s,a) = \mathbb{E}_{s' \sim \mathcal{E}} [r+\gamma \max_{a'}Q^*(s',a')|s,a]
</span><script type="math/tex; mode=display">
Q^*(s,a) = \mathbb{E}_{s' \sim \mathcal{E}} [r+\gamma \max_{a'}Q^*(s',a')|s,a]
</script></span></p>
<p>In the case of Deep Q-Learning this is performed by minizing a sequence of loss functions <span><span class="MathJax_Preview">L_i(\theta_i)</span><script type="math/tex">L_i(\theta_i)</script></span> which change at each iteration <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> of the algorithm:
<span><span class="MathJax_Preview">
L_i(\theta_i) = \mathbb{E}_{s', a \sim \rho(\cdot)} [ g(y_i - Q(s,a;\theta_i)) ]
</span><script type="math/tex; mode=display">
L_i(\theta_i) = \mathbb{E}_{s', a \sim \rho(\cdot)} [ g(y_i - Q(s,a;\theta_i)) ]
</script></span>
where <span><span class="MathJax_Preview">g : \mathbb{R}^n\rightarrow\mathbb{R}</span><script type="math/tex">g : \mathbb{R}^n\rightarrow\mathbb{R}</script></span> is some loss function, usually <a href="https://en.wikipedia.org/wiki/Mean_squared_error">L2</a> or <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber-loss</a> and <span><span class="MathJax_Preview">y_i= \mathbb{E}_{s' \sim \rho(\cdot)} [r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) ]</span><script type="math/tex">y_i= \mathbb{E}_{s' \sim \rho(\cdot)} [r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) ]</script></span> is the <em>target</em>. The policy is updated with stochastic gradient descent where stochastic gardients are sampled from the following full gradient computation:
<span><span class="MathJax_Preview">
\nabla L_i(\theta_i) = \mathbb{E}_{s' \sim \epsilon} [ g'(r + \gamma \max_{a'} Q(s',a';\theta_{i-1})-
Q(s,a;\theta_i)) \cdot \nabla_{\theta_i} Q(s,a;\theta_i)) ]
</span><script type="math/tex; mode=display">
\nabla L_i(\theta_i) = \mathbb{E}_{s' \sim \epsilon} [ g'(r + \gamma \max_{a'} Q(s',a';\theta_{i-1})-
Q(s,a;\theta_i)) \cdot \nabla_{\theta_i} Q(s,a;\theta_i)) ]
</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>_type_</code></dt>
<dd>the simulation environment.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>_type_</code></dt>
<dd>the torch module to use for learning</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>nn._Loss</code>, optional</dt>
<dd>the loss function. Defaults to nn.HuberLoss().</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>DQN's learning rate. Defaults to 5e-4.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>the exploration ratio epsilon (see above). Defaults to 0.5.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>the gamma term (see above). Defaults to 0.99.</dd>
<dt><strong><code>buffer_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the replay buffer size. Defaults to 10000.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the size of each training batch. Defaults to 64.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DQNAgent(Agent):
    &#34;&#34;&#34;Implements acting and learning using deep Q-learning (see [the DQN paper](https://arxiv.org/pdf/1312.5602.pdf)).  
    
    Q-learning aims to optmizes an agent&#39;s policy by maximizing the Bellman equation:
    $$
    Q^*(s,a) = \mathbb{E}_{s&#39; \sim \mathcal{E}} [r+\gamma \max_{a&#39;}Q^*(s&#39;,a&#39;)|s,a]
    $$
    
    In the case of Deep Q-Learning this is performed by minizing a sequence of loss functions \(L_i(\\theta_i)\) which change at each iteration \(i\) of the algorithm:
    $$
    L_i(\\theta_i) = \mathbb{E}_{s&#39;, a \sim \\rho(\cdot)} [ g(y_i - Q(s,a;\\theta_i)) ]
    $$
    where \(g : \mathbb{R}^n\\rightarrow\mathbb{R}\) is some loss function, usually [L2](https://en.wikipedia.org/wiki/Mean_squared_error) or [Huber-loss](https://en.wikipedia.org/wiki/Huber_loss) and \(y_i= \mathbb{E}_{s&#39; \sim \\rho(\cdot)} [r + \gamma \max_{a&#39;} Q(s&#39;,a&#39;;\\theta_{i-1}) ]\) is the *target*. The policy is updated with stochastic gradient descent where stochastic gardients are sampled from the following full gradient computation:
    $$
    \\nabla L_i(\\theta_i) = \mathbb{E}_{s&#39; \sim \epsilon} [ g&#39;(r + \gamma \max_{a&#39;} Q(s&#39;,a&#39;;\\theta_{i-1})-  Q(s,a;\\theta_i)) \cdot \\nabla_{\\theta_i} Q(s,a;\\theta_i)) ]
    $$
    
    
    &#34;&#34;&#34;

    def __init__(self,  env,
                 model:torch.nn,
                 criterion=nn.HuberLoss(),
                 lr:float=5e-4,
                 epsilon:float=0.5,
                 gamma:float=0.99,
                 buffer_size:int=10000,
                 batch_size:int=64):
        &#34;&#34;&#34;

        Args:
            env (_type_): the simulation environment.
            model (_type_): the torch module to use for learning
            criterion (nn._Loss, optional): the loss function. Defaults to nn.HuberLoss().
            lr (float, optional): DQN&#39;s learning rate. Defaults to 5e-4.
            epsilon (float, optional): the exploration ratio epsilon (see above). Defaults to 0.5.
            gamma (float, optional): the gamma term (see above). Defaults to 0.99.
            buffer_size (int, optional): the replay buffer size. Defaults to 10000.
            batch_size (int, optional): the size of each training batch. Defaults to 64.
        &#34;&#34;&#34;

        self.env = env

        model_params = {
            &#39;in_dim&#39;: len(env.observation_space.sample().flatten()),
            &#39;out_dim&#39;: env.action_space.n,
        }
        self.model = model(**model_params)
        self.targetModel = model(**model_params)

        self.criterion = criterion

        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)

        self.device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        self.memory = ReplayMemory(buffer_size)
        self.batch_size = batch_size

        self.epsilon = epsilon
        self.gamma = gamma
        self.lr = lr

    def load_model(self, savepath):
        torch.save(self.model.state_dict(), savepath)
        self.model.load_state_dict(torch.load(savepath))

    def save_model(self, savepath):
        torch.save(self.model.state_dict(), savepath)

    def optimize_model(self):
        if len(self.memory) &lt; self.batch_size:
            return np.double(0)

        # Sample memory
        transitions = self.memory.sample(self.batch_size)

        # Convert Batch(Transitions) -&gt; Transition(Batch)
        batch = Transition(*zip(*transitions))

        action_batch = torch.tensor([e for e in batch.action])
        state_batch = torch.cat(batch.state, 0)
        next_states_batch = torch.cat(batch.next_state, 0)
        reward_batch = torch.cat(batch.reward)

        # Compute Q(S, a) with the Q-value network
        state_action_values = self.model(
            state_batch).gather(1, action_batch.unsqueeze(1))

        # Compute max_ap Q(Sp) with the stable target network
        next_state_values = self.targetModel(next_states_batch).max(1)[
            0].detach().unsqueeze(1)
        # Compute the expected Q values
        expected_state_action_values = (
            next_state_values * self.gamma) + reward_batch

        # Compute Huber loss
        loss = self.criterion(state_action_values,
                              expected_state_action_values)

        # Optimize the model
        self.optimizer.zero_grad()
        loss.backward()
        for param in self.model.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()

        return np.double(loss)
    
    def reset():
        pass # Not stateful

    def act(self, obs):
        x = torch.Tensor(obs)

        epsilon = self.epsilon
        sample = random.random()

        Q_est = self.model(x)
        Q = float(Q_est.detach().max())
        if sample &gt; epsilon:
            with torch.no_grad():
                return np.argmax(
                    np.exp(Q_est)), Q
        else:
            return self.env.action_space.sample(), Q</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deep_q_learning.agent.Agent" href="#deep_q_learning.agent.Agent">Agent</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deep_q_learning.agent.Agent" href="#deep_q_learning.agent.Agent">Agent</a></b></code>:
<ul class="hlist">
<li><code><a title="deep_q_learning.agent.Agent.act" href="#deep_q_learning.agent.Agent.act">act</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.load_model" href="#deep_q_learning.agent.Agent.load_model">load_model</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.optimize_model" href="#deep_q_learning.agent.Agent.optimize_model">optimize_model</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.reset" href="#deep_q_learning.agent.Agent.reset">reset</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.save_model" href="#deep_q_learning.agent.Agent.save_model">save_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="deep_q_learning.agent.NaiveAgent"><code class="flex name class">
<span>class <span class="ident">NaiveAgent</span></span>
<span>(</span><span>env, threshold: int = 20000, confine_time_int=4)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements acting and learning. (Abstract class, for implementations see DQNAgent and NaiveAgent).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ABC</code></strong> :&ensp;<code>_type_</code></dt>
<dd><em>description</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl>
<p>Naive Agent implementation. Gives a baseline to compare reinforcement learning agents against.
The naive policy is the following:</p>
<pre><code class="language-pseudocode">If number of infected people &gt; THRESHOLD
    confine the entire country for CONFINEMENT_TIME weeks
</code></pre>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>_type_</code></dt>
<dd>the simulation environment.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The infected threshold, upon which confiment must start. Defaults to 20000.</dd>
<dt><strong><code>confine_time</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The confinement time. Defaults to 4.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NaiveAgent(Agent):

    def __init__(self,  env,
                 threshold:int=20000,
                 confine_time_int=4,):
        &#34;&#34;&#34;Naive Agent implementation. Gives a baseline to compare reinforcement learning agents against. 
        The naive policy is the following:
        ```pseudocode
        If number of infected people &gt; THRESHOLD
            confine the entire country for CONFINEMENT_TIME weeks
        ```

        Args:
            env (_type_): the simulation environment.
            threshold (int, optional): The infected threshold, upon which confiment must start. Defaults to 20000.
            confine_time (int, optional): The confinement time. Defaults to 4.
        &#34;&#34;&#34;

        self.env = env
        self.threshold = threshold
        self.confine_time = confine_time
        self.timer = 0
    def load_model(self, savepath):
        pass

    def save_model(self, savepath):
        pass

    def optimize_model(self):
        #This is agent is born stupid and stays stupid
        return 0
    
    def reset(self,):
        self.timer = 0

    def act(self, obs):
        if self.timer &gt; 0:
            self.timer -=1
            return 1, 0
        if obs &gt; self.threshold:
            self.timer = self.confine_time
            return 1, 0
        return 0,0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deep_q_learning.agent.Agent" href="#deep_q_learning.agent.Agent">Agent</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deep_q_learning.agent.Agent" href="#deep_q_learning.agent.Agent">Agent</a></b></code>:
<ul class="hlist">
<li><code><a title="deep_q_learning.agent.Agent.act" href="#deep_q_learning.agent.Agent.act">act</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.load_model" href="#deep_q_learning.agent.Agent.load_model">load_model</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.optimize_model" href="#deep_q_learning.agent.Agent.optimize_model">optimize_model</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.reset" href="#deep_q_learning.agent.Agent.reset">reset</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.save_model" href="#deep_q_learning.agent.Agent.save_model">save_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="deep_q_learning.agent.ReplayMemory"><code class="flex name class">
<span>class <span class="ident">ReplayMemory</span></span>
<span>(</span><span>capacity)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        &#34;&#34;&#34;Save a transition&#34;&#34;&#34;
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="deep_q_learning.agent.ReplayMemory.push"><code class="name flex">
<span>def <span class="ident">push</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Save a transition</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push(self, *args):
    &#34;&#34;&#34;Save a transition&#34;&#34;&#34;
    self.memory.append(Transition(*args))</code></pre>
</details>
</dd>
<dt id="deep_q_learning.agent.ReplayMemory.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self, batch_size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self, batch_size):
    return random.sample(self.memory, batch_size)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="deep_q_learning.agent.Transition"><code class="flex name class">
<span>class <span class="ident">Transition</span></span>
<span>(</span><span>state, action, next_state, reward)</span>
</code></dt>
<dd>
<div class="desc"><p>Transition(state, action, next_state, reward)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="deep_q_learning.agent.Transition.action"><code class="name">var <span class="ident">action</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="deep_q_learning.agent.Transition.next_state"><code class="name">var <span class="ident">next_state</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="deep_q_learning.agent.Transition.reward"><code class="name">var <span class="ident">reward</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="deep_q_learning.agent.Transition.state"><code class="name">var <span class="ident">state</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="deep_q_learning" href="index.html">deep_q_learning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="deep_q_learning.agent.Agent" href="#deep_q_learning.agent.Agent">Agent</a></code></h4>
<ul class="">
<li><code><a title="deep_q_learning.agent.Agent.act" href="#deep_q_learning.agent.Agent.act">act</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.load_model" href="#deep_q_learning.agent.Agent.load_model">load_model</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.optimize_model" href="#deep_q_learning.agent.Agent.optimize_model">optimize_model</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.reset" href="#deep_q_learning.agent.Agent.reset">reset</a></code></li>
<li><code><a title="deep_q_learning.agent.Agent.save_model" href="#deep_q_learning.agent.Agent.save_model">save_model</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deep_q_learning.agent.DQNAgent" href="#deep_q_learning.agent.DQNAgent">DQNAgent</a></code></h4>
</li>
<li>
<h4><code><a title="deep_q_learning.agent.NaiveAgent" href="#deep_q_learning.agent.NaiveAgent">NaiveAgent</a></code></h4>
</li>
<li>
<h4><code><a title="deep_q_learning.agent.ReplayMemory" href="#deep_q_learning.agent.ReplayMemory">ReplayMemory</a></code></h4>
<ul class="">
<li><code><a title="deep_q_learning.agent.ReplayMemory.push" href="#deep_q_learning.agent.ReplayMemory.push">push</a></code></li>
<li><code><a title="deep_q_learning.agent.ReplayMemory.sample" href="#deep_q_learning.agent.ReplayMemory.sample">sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deep_q_learning.agent.Transition" href="#deep_q_learning.agent.Transition">Transition</a></code></h4>
<ul class="">
<li><code><a title="deep_q_learning.agent.Transition.action" href="#deep_q_learning.agent.Transition.action">action</a></code></li>
<li><code><a title="deep_q_learning.agent.Transition.next_state" href="#deep_q_learning.agent.Transition.next_state">next_state</a></code></li>
<li><code><a title="deep_q_learning.agent.Transition.reward" href="#deep_q_learning.agent.Transition.reward">reward</a></code></li>
<li><code><a title="deep_q_learning.agent.Transition.state" href="#deep_q_learning.agent.Transition.state">state</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>